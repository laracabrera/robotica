---
marp : true
auto-scaling:
    - true
    - fittingHeader
    - math
    - code
paginate : true
theme : hegel
title : OptimizaciÃ³n mediante algoritmos por refuerzo
author :
   - Alberto DÃ­az Ãlvarez <alberto.diaz@upm.es>
   - Raul Lara Cabrera <raul.lara@upm.es>
description : OptimizaciÃ³n. Curso 2022-2023. E.T.S.I. Sistemas InformÃ¡ticos (UPM)
math: katex
---

<!-- _class: titlepage -->
![bg left:33% width:100%](https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg)

<div class="title">OptimizaciÃ³n mediante algoritmos por refuerzo</div>
<div class="subtitle">RobÃ³tica</div>
<div class="author">Alberto DÃ­az y RaÃºl Lara</div>
<div class="date">Curso 2022/2023</div>
<div class="organization">Departamento de Sistemas InformÃ¡ticos</div>

[![height:30](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-informational.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

---

# Paradigmas de aprendizaje en <i>Machine Learning</i>

**Supervisado**: Se aprende de ejemplos con sus correspondientes respuestas.

- Problemas de regresiÃ³n y clasificaciÃ³n.

**No supervisado**: BÃºsqueda de patrones en datos no etiquetados.

- Problemas de <i>clustering</i>, reducciÃ³n de la dimensionalidad, recodificaciÃ³n, ...

**Por refuerzo**: Se aprende a travÃ©s de la experiencia a base de recompensas.

- Problemas de aprendizaje de polÃ­ticas de decisiÃ³n.

---

<!-- _class: cite -->
 
<div class="cite-author" data-text="Edward Thorndike - Law of Effect">

   "De varias respuestas dadas al mismo hecho, las seguidas de satisfacciÃ³n para el animal estarÃ¡n, en igualdad de condiciones, mÃ¡s firmemente conectadas con este, de modo que tenderÃ¡n a repetirse; las seguidas de incomodidad para el animal tendrÃ¡n, en igualdad de condiciones, sus conexiones con el hecho debilitadas, de modo que tenderÃ¡n a ocurrir menos. Cuanto mayor sea la satisfacciÃ³n o el malestar, mayor serÃ¡ el refuerzo o el deterioro del vÃ­nculo."

</div>

---

# Aprendizaje por refuerzo (RL)

Subcampo del <i>machine learning</i> donde los agentes aprenden interactuando:

- **Imita** de manera fundamental el **aprendizaje** de muchos **seres vivos**.
- Esa interacciÃ³n produce tanto resultados deseados como no deseados.
- Se entrena con la **recompensa o castigo** determinados para dicho resultado.
- El agente tratarÃ¡ de maximizar la recompensa a largo plazo.

Se utiliza principalmente en dos Ã¡reas hoy en dÃ­a:

- **Juegos**: Los agentes aprenden las reglas y las jugadas jugando.
- **Control**: Los agentes aprenden en entornos de simulaciÃ³n las mejores polÃ­ticas de control para un problema determinado.

> Un ejemplo curioso es el publicado en <https://www.nature.com/articles/nature14236>, donde describen cÃ³mo un agente aprende a jugar a 49 juegos de Atari 2600 llegando a un nivel de destreza comparable al humano.

---

# TerminologÃ­a

**Agente inteligente**: Entidad que interactÃºa con el entorno.

- En este contexto, se usarÃ¡n indistintamente los conceptos de agente, agente inteligente y robot.

**Entorno**: El entorno es el mundo en el que se mueve el agente.

- Puede ser fÃ­sico o virtual.
- Posee un conjunto de reglas.
- Se suele suponer que el entorno es determinista.

El agente tiene un estado en el entorno.

- Realiza acciones que cambian su estado en dicho entorno.

---

# Ejemplo: El juego del Go

El entorno es el tablero de Go

El agente es el robot que juega al Go

El estado del entorno es la colocaciÃ³n de las piedras.

Dado un estado concreto, solo hay un nÃºmero finito de movimientos legales (acciones) que se pueden hacer

- Estos estÃ¡n determinados por el entorno.
- Por ejemplo, no se puede poner una piedra en un lugar ocupado.

Cuando el agente realiza una acciÃ³n (poner piedra), el entorno la recibe como entrada y genera un estado resultato y una recompensa

---

# Modelo de interacciÃ³n agente-entorno

**Comportamiento**: SucesiÃ³n de estados, acciones y recompensas asociadas:

$$(s_1, a_1, r_2), (s_2, a_2, r_3), \ldots, (s_i, a_i, r_{i+1}), \ldots$$

<img src="../img/t4/Modelo%20agente-entorno.svg">

Definimos:

- Probabilidad de transiciÃ³n: $P(s_{t+1} | (s_t, a_t), (s_{t-1}, a_{t-1}), \ldots, (s_1, a_1))$
- Probabilidad de transiciÃ³n: $P(r_{t+1} | (s_t, a_t), (s_{t-1}, a_{t-1}), \ldots, (s_1, a_1))$

---

# Proceso de aprendizaje por refuerzo

El proceso implica los siguientes pasos:

1. Observar el entorno en el que se encuentra el agente.
2. Decidir quÃ© acciÃ³n tomar usando alguna estrategia de toma de decisiones.
3. Ejecutar dicha acciÃ³n.
4. Recibir una recompensa o castigo en funciÃ³n de la acciÃ³n tomada.
5. Aprender de la experiencia y perfeccionar la estrategia de toma de decisiones.
6. Iterar todo el proceso hasta encontrar una estrategia Ã³ptima.

---

# Ejemplo: Hambre y zombies

Objetivo: Utilizar tÃ©cnicas de RL para que el superviviente llegue a su destino.

<hr>
<div style="margin: 0 auto;">
<pre>
ğŸ¢ğŸ—»ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢
ğŸ¢â¬›â¬›â¬›ğŸ§ŸğŸ¢â¬›â¬›â¬›ğŸ§Ÿ
ğŸ¢â¬›ğŸ¢â¬›ğŸ¢ğŸ¢â¬›ğŸ¢ğŸ¢ğŸ¢
ğŸ¢â¬›ğŸ¢ğŸ¥˜ğŸ¢ğŸ¥˜â¬›ğŸ”ğŸ”ğŸ”
ğŸ¢â¬›ğŸ¢ğŸ¢ğŸ¢ğŸ¢â¬›ğŸ¢ğŸ¢ğŸ¢
ğŸ¢â¬›ğŸ”ğŸ¢â¬›â¬›â¬›â¬›â¬›ğŸ¢
ğŸ¢â¬›â¬›â¬›â¬›ğŸ¢ğŸ§ŸğŸ¢â¬›ğŸ¢
ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¤°ğŸ¢
</pre>
</div>
<hr>

Hay que comenzar considerando los estados, las acciones y las recompensas.

---

# Estados

El agente se encuentra en un estado y toma una acciÃ³n de acuerdo a este.

Espacio de estados: Todas las situaciones posibles en las que se puede encontrar el agente.

- Debe contener informaciÃ³n suficiente para tomar una decisiÃ³n correcta.

En el ejemplo, son todas las posiciones que podrÃ­a ocupar el agente (35).

- PodrÃ­amos complicarlo mÃ¡s, por ejemplo, obligando a llevar comida.
- Esto implicarÃ­a los 35 estados con y sin comida encima (35 + 35 = 70).
- Pero nos quedaremos con el ejemplo simple.

---

# Acciones

El agente se encuentra con uno de los 35 estados y realiza una acciÃ³n.

- 5 acciones posibles: arriba, abajo, izquierda, derecha y coger comida.

**Espacio de acciones**: Conjunto de todas las acciones posibles para un estado.

---

# Recompensas

El superviviente estÃ¡ motivado por la recompensa, asÃ­ que aprenderÃ¡ a:

- Encontrar la comida y el objetivo.
- Evitar las zonas infestadas de zombies.

Algunos puntos a tener en cuenta para el agente:

- Alta recompensa por llegar a las montaÃ±as ğŸ—» (+1000); es el objetivo.
- Ligera recompensa por encontrar comida ğŸ¥˜ğŸ” (+10) porque estÃ¡ bien.
- PenalizaciÃ³n si llega a un zombie ğŸ§Ÿ (-50) porque no interesa en absoluto.

Es importante tener en cuenta que la recompensa no siempre es inmediata:

- Puede haber tramos sin nada hasta llegar a un estado muy bueno.

---

# <i>OpenAI Gym</i><!--_class: transition-->

---

[CREAR EL ENTORNO DE OPENAI GYM PARA EL EJEMPLO: <https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e>]

---

# El entorno <i>OpenAI Gym</i>

OpenAI Gym es una biblioteca de entornos de aprendizaje por refuerzo.

- Proporciona entornos de juego para probar nuestros agentes desarrollados.

Se encarga de proporcionar toda la informaciÃ³n que el agente necesitarÃ­a:

- Entorno, posibles acciones y recompensas, estado actual, ...
- SÃ³lo tenemos que preocuparnos de la lÃ³gica del agente.

El ejemplo anterior se ha implementado como entorno para practicar con Ã©l.

---

# InstalaciÃ³n

La biblioteca estÃ¡ disponible a travÃ©s de Pypi:

```bash
pip install gym
```

Una vez instalada, podemos cargar el entorno del juego y mostrar su aspecto:

```python
import gym

env = gym.make("Starvation and Zombies").env
env.render()
```

---

# Propiedad de MÃ¡rkov

El estado futuro del proceso depende del estado actual, y no de los anteriores.

- Es un estado que cumplen ciertos procesos estocÃ¡sticos.
- Definida por AndrÃ©i Markov en 1906 en su TeorÃ­a de Cadenas de MÃ¡rkov.

Al proceso que satisface esta propiedad se denomina **Proceso de MÃ¡rkov**.

- Concretamente se denominan Procesos de MÃ¡rkov de **primer orden**.
- La definiciÃ³n se puede extender a $n$ estados anteriores (proceso de orden $n$).

Los conceptos cadena de MÃ¡rkov y proceso Markov se usan indistintamente cuando el espacio de estados del proceso este es discreto.

---

# Procesos de decisiÃ³n de Markov (MDP) en agentes

Se asume que el proceso de decisiÃ³n de un agente es un MDP:

- $P(s_{t+1} | (s_t, a_t), (s_{t-1}, a_{t-1}), \ldots, (s_1, a_1)) = P(s_{t+1} | (s_t, a_t))$
- $P(s_{r+1} | (s_t, a_t), (s_{t-1}, a_{t-1}), \ldots, (s_1, a_1)) = P(r_{t+1} | (s_t, a_t))$

Los procesos de decisiÃ³n de un 

Sistemas de toma de decisiones basados en procesos de MÃ¡rkov. Incluyen:

- $S$: Conjunto finito de estados.
- $A$: Conjunto finito de acciones.
- $P(s_i|(s_j, a)$: Probabilidad de transiciÃ³n de $s_i$ a $s_j$ con la acciÃ³n $a$.
- $\pi : S \rightarrow A$: FunciÃ³n que define las polÃ­ticas de decisiÃ³n.
- 
- **Transiciones** entre estados.
- **Recompensas** por transiciÃ³n. Pueden ser positivas o negativas.
- Factor de descuento $\gamma \in [0, 1]$: Importancia entre recompensas inmediatas o futuras (generalmente $\gamma^t$). <!-- Si por ejemplo gamma es 0.9 y hay una recompensa de 100 a 5 pasos del punto en el que estamos, la recompensa realmente serÃ¡ de 100 * 0.9^5 -->
- Memoria: En orden 1 no es necesaria memoria pero en Ã³rdenes mayores sÃ­.

---

# MDP en nuestro ejemplo

El objetivo del superviviente es intentar maximizar la suma de las recompensas futuras tomando la mejor acciÃ³n para cada estado:

$$\sum_{t=0}^\infty r_{e_t, a_t} \cdot \gamma^t$$

Explicado:

1. Estamos sumando para cada paso de tiempo $t$, de ahÃ­ el sumatorio.
2. Cada paso de tiempo tiene una recompensa $r_{e_t, a_t}$ asociada la acciÃ³n tomada.
3. $\gamma^t$ es el factor de descuento en 1 por ahora y olvidÃ©monos de ello.

Una vez formalizado el problema, vamos a explorar algunas soluciones.

---

# SoluciÃ³n #1: Q-learning

Se apoya en una funciÃ³n denominada  acciÃ³n-valor (<i>action-value</i>) o funciÃ³n $Q$:

- Entrada: Estado y acciÃ³n a realizar.
- Salida: Recompensa esperada de esa acciÃ³n (y de todas las posteriores).

La funciÃ³n $Q$ se actualiza de forma iterativa:

1. Antes de explorar el entorno, $Q$ da el mismo valor fijo (arbitrario).
2. SegÃºn se explora, aproxima mejor el valor de la acciÃ³n $a$ en un estado $s$.
3. SegÃºn se avanza, la funciÃ³n $Q$ se actualiza.

Representa suma de las recompensas de elegir la acciÃ³n $Q$ y todas las acciones Ã³ptimas posteriores.

---

$$Q(e_t, a_t) = Q(e_t, a_t) + \alpha \cdot (r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t))$$

Realizar $a_t$ en el estado $e_t$ actualiza su valor con un tÃ©rmino que contiene:

- $\alpha$: Lo "agresivo" que sstamos haciendo el entrenamiento.
- $r_t$: EstimaciÃ³n que obtuvimos al actuar en el estado $e_t$ anteriormente.
- $\max_a Q(s_{t+1}, a)$: Recompensa futura estimada.
- Se resta ademÃ¡s el valor antigÃ¼o para incrementar o disminuir la diferencia en la estimaciÃ³n.

Ahora tenemos una estimaciÃ³n de valor para cada par estado-acciÃ³n.

- Con el podemos elegir la acciÃ³n que nos interesa (e.g. usando epsilon-greedy)

---

# Estrategia epsilon-greedy

Estrategia muy sencilla para evitar mÃ­nimos locales:

- El agente elige una acciÃ³n de forma aleatoria con probabilidad $\epsilon$
- La mejor acciÃ³n conocida con probabilidad $1-\epsilon$.

Por lo general, se empieza con Ã©psilon alto (mucha exploraciÃ³n).

- SegÃºn el superviviente aprende mÃ¡s sobre la ciudad, epsilon disminuye.
- Ha explorado mucho, asÃ­ que puede centrarse en explotar lo conocido.

---

# SoluciÃ³n #2: <i>Policy learning</i>

Trata de determinar una funciÃ³n $\pi$ asigna la mejor acciÃ³n a un estado dado:

$$a = \pi(e)$$

<cite>"Cuando observo el estado $e$, lo mejor que puedo hacer es tomar la acciÃ³n $a$"</cite>

Esta funciÃ³n es una funciÃ³n compleja que tratamos de aproximar.

- Y lo mÃ¡s "sencillo" y rÃ¡pido es usar redes neuronales para ello.

---

# Otras soluciones

## <i>Deep $Q$-networks</i> (DQN)

Son aproximaciones de funciones $Q$ utilizando redes neuronales profundas<sup>2</sup>.

## Asynchronous Advantage Actor-Critic (A3C)

Es una combinaciÃ³n de las dos tÃ©cnicas anteriores<sup>3</sup>, combinando:

- Un actor: Red de polÃ­ticas de actuaciÃ³n que deciden quÃ© acciÃ³n tomar.
- Un crÃ­tico: DQN que decide el valor de cada acciÃ³n a tomar.

> <sup>2</sup> <https://www.nature.com/articles/nature14236>
> <sup>3</sup> <https://proceedings.mlr.press/v48/mniha16.html>

---

<video controls width=100% src="https://drive.upm.es/s/0VIKqV7AiEQSzPu/download"></video>

---

# Relevancia del aprendizaje por refuerzo<!--_class: transition-->

---

<!-- _class: cite -->
 
<div class="cite-author">

   "El Go es un juego estudiado por los humanos durante mÃ¡s de 2500 aÃ±os. AlphaZero, en un tiempo insignificante (3 dÃ­as), pasÃ³ de conocer sÃ³lo las reglas del juego a vencer a los mejores jugadores del mundo, superando todo nuestro conocimiento acumulado durante milenios. NingÃºn campo del aprendizaje automÃ¡tico ha permitido avanzar tanto en este tipo de problemas como el aprendizaje por refuerzo."

</div>

---

# Relevancia del aprendizaje por refuerzo hoy en dÃ­a

Podemos decir que es prÃ¡cticamente el Ãºnico paradigma de aprendizaje:

- Capaz de aprender comportamientos complejos en entornos complejos.
- Que ha podido hacerlo prÃ¡cticamente sin supervisiÃ³n humana.

Ofrece a la robÃ³tica forma abordar cÃ³mo diseÃ±ar comportamientos difÃ­ciles.

- Que por otro lado, son prÃ¡cticamente todos.
- Las cosas fÃ¡ciles para un humano suelen ser las mÃ¡s complejas de diseÃ±ar.

Permite a robots descubrir de forma autÃ³noma comportamientos Ã³ptimos:

- No se detalla la soluciÃ³n al problema, sino que se interacciona con el entorno.
- La retroalimentaciÃ³n de el efecto sobre el entorno permite aprender.

---

# La utilidad de los modelos aproximados

Los datos del mundo real pueden usarse para aprender modelos aproximados.

- Mejor, porque el proceso de aprendizaje por ensayo y error es muy lento.
- Sobre todo en un sistema que tiene que hacerlo en un entorno fÃ­sico.
- Las simulaciones suelen ser mucho mÃ¡s rÃ¡pidas que el tiempo real.
- Y tambiÃ©n tambiÃ©n mucho mÃ¡s seguras para el robot y el entorno
- <i>**Mental rehearsal**</i>: Describe el proceso de aprendizaje en simulaciÃ³n.

Suele ocurrir que un modelo aprende en simulaciÃ³n pero falla en la realidad:

- Esto se conoce como **sesgo de simulaciÃ³n**.
- Es anÃ¡logo al sobreajuste en el aprendizaje supervisado.
- Se ha demostrado que puede abordarse introduciendo modelos estocÃ¡sticos. <!-- Incluso si el sistema es muy cercano al determinismo. -->

---

# Impacto del uso de conocimiento o informaciÃ³n previa

El conocimiento previo puede ayudar a guiar el proceso de aprendizaje:

- Este enfoque reduce significativamente el espacio de bÃºsqueda.
- Esto produce una **aceleraciÃ³n** dramÃ¡tica **en el proceso de aprendizaje**.
- TambiÃ©n **reduce la posibilidad de encontrar mejores Ã³ptimos**<sup>1</sup>.

Existen dos tÃ©cnicas principales para introducir conocimiento previo:

- A travÃ©s de la **demostraciÃ³n**: Se da una polÃ­tica inicial semi-exitosa.
- A travÃ©s de la **estructuraciÃ³n de la tarea**: Se da la tarea dividida.

> <sup>1</sup> Alpha Go fue entrenado con un conocimiento previo de Go, pero Alpha Go Zero no sabÃ­a nada del juego. El resultado fue que Alpha Go Zero jugÃ³ y ganÃ³ a Alpha Go en 100 partidas.
---

# DesafÃ­os del aprendizaje por refuerzo

**La maldiciÃ³n de la dimensionalidad**: El espacio de bÃºsqueda crece exponencialmente con el nÃºmero de estados.

**La maldiciÃ³n del mundo real**: El mundo real es muy complejo y no se puede simular.

- Desgaste, estocasticidad, cambios de dinÃ¡mica, intensidad de la luz, ...

**La maldiciÃ³n de la incertidumbre del modelo**: El modelo no es perfecto y no se puede simular.

- Cada pequeÃ±o error se acumula, haciendo que conseguir un modelo suficientemente preciso del robot y su entorno sea un reto

---

# Algunas reflexiones

Â¿Nosotros como humanos tenemos funciones de valor? Â¿CÃ³mo definimos la recompensa que maximizamos en nuestra vida real?

- MÃ¡s allÃ¡ del placer y el dolor, tiende a incluir otros elementos complicados como el bien y el mal, la satisfacciÃ³n, el amor, la espiritualidad, etc.

La pregunta central de la filosofÃ­a moral es: Â¿quÃ© debemos hacer?

- Â¿CÃ³mo debemos vivir? Â¿QuÃ© acciones son correctas o incorrectas?
- Una posible respuesta es que, claramente, depende de los valores de cada uno.

A medida que vayamos creando una IA cada vez mÃ¡s avanzada, Ã©sta empezarÃ¡ a salir de los problemas donde la recompensa se define mediante un nÃºmero de puntos ganados en el juego, y requerirÃ¡ recompensas mÃ¡s complejas.

- Los vehÃ­culos autÃ³nomos, por ejemplo, son agentes que tienen que tomar decisiones con una definiciÃ³n de recompensa algo mÃ¡s compleja
- Al principio, la recompensa podrÃ­a estar ligada a algo como "llegar a salvo al destino".
- Pero Â¿y si se ve obligado a elegir entre mantener el rumbo y atropellar a cinco peatones o desviarse y atropellar a uno? Â¿debe desviarse o incluso daÃ±ar al conductor con una maniobra peligrosa? Â¿Y si el Ãºnico peatÃ³n es un niÃ±o, o un anciano, o el prÃ³ximo Einstein o Hitler? Â¿Cambia eso la decisiÃ³n? Â¿por quÃ©? Â¿Y si al dar un volantazo tambiÃ©n se destruimos una escultura extremadamente valiosa e irremplazable?
- De repente tenemos un problema mucho mÃ¡s complejo cuando intentamos definir la funciÃ³n objetivo, y las respuestas no son tan sencillas.

Nosotros, como humanos, Â¿cÃ³mo sabemos lo que es correcto o no? Generalmente podemos responder "por intuiciÃ³n".

- Pero ponerlo en palabras o reglas es sencillamente imposible.
- Sin embargo, quizÃ¡s sea posible que una mÃ¡quina aprenda estos valores, esta "intuiciÃ³n", de alguna manera.
- Este puede ser uno de los problemas tÃ©cnicos mÃ¡s importantes que los humanos tendrÃ¡n que resolver en un futuro.

---

# Â¡GRACIAS!<!--_class: transition-->
